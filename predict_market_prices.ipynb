{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predict_market_prices.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNj+XjNC+9gfZIJ25mUbDjd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kconstable/market_predictions/blob/main/predict_market_prices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTF4aoPHf1rl"
      },
      "source": [
        "# Predict Market Prices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWKsQM4qHRYT"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjLmv9qcf0XK",
        "outputId": "7a822e72-105b-4674-a235-a8d5ee727b35"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import time\n",
        "import pickle\n",
        "import re\n",
        "import datetime\n",
        "from datetime import timedelta\n",
        "from tabulate import tabulate\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "\n",
        "# alphavalue key\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/data/av_key.txt') as f:\n",
        "    key = f.read().strip()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning:\n",
            "\n",
            "pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwBqRIum9SRx"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aZ4ysO3hC08"
      },
      "source": [
        "def get_economic_indicators(funct,key,interval=None,maturity=None,throttle=0):\n",
        "  \"\"\"\n",
        "  Returns Economic Indicator Data with missing values interpolated between dates\n",
        "  Monthly Data:\n",
        "    NONFARM_PAYROLL, INFLATION_EXPECTATION,CONSUMER_SENTIMENT,UNEMPLOYMENT\n",
        "  Daily, Weekly, Monthly Data:  \n",
        "    FEDERAL_FUNDS_RATE = interval (daily,weekly,monthly)\n",
        "    TREASURY_YIELD = interval (daily, weekly, monthly), \n",
        "                     maturity (3month, 5year, 10year, and 30year)\n",
        "  \"\"\"\n",
        "  \n",
        "  # query strings\n",
        "  # Monthly Data:\n",
        "  if funct in ['NONFARM_PAYROLL','INFLATION_EXPECTATION','CONSUMER_SENTIMENT','UNEMPLOYMENT']:\n",
        "    url = f'https://www.alphavantage.co/query?function={funct}&apikey={key}'\n",
        "\n",
        "  # Daily, Weekly or Monthly Data:\n",
        "  # Interest Rates\n",
        "  if funct == 'FEDERAL_FUNDS_RATE':\n",
        "    url = f'https://www.alphavantage.co/query?function={funct}&interval={interval}&apikey={key}'\n",
        "\n",
        "  # Treasury Yield  \n",
        "  if funct == 'TREASURY_YIELD':\n",
        "    url = f'https://www.alphavantage.co/query?function={funct}&interval={interval}&maturity={maturity}&apikey={key}'\n",
        "\n",
        "  # pull data\n",
        "  r = requests.get(url)\n",
        "  time.sleep(throttle)\n",
        "  d = r.json()\n",
        "\n",
        "  # convert to df\n",
        "  df = pd.DataFrame(d['data'])\n",
        "\n",
        "  # move date to a datetime index\n",
        "  df.date = pd.to_datetime(df.date)\n",
        "  df.set_index('date',inplace=True)\n",
        "\n",
        "  # add the ticker name and frequency\n",
        "  df['name'] = d['name']\n",
        "  df['interval']=d['interval'] \n",
        "\n",
        "  # clean data & interpolate missing values\n",
        "  # missing data encoded with '.'\n",
        "  # change datatype to float\n",
        "  df.replace('.',np.nan,inplace=True)\n",
        "  df.value = df.value.astype('float')\n",
        "\n",
        "  # missing data stats\n",
        "  missing =sum(df.value.isna())\n",
        "  total =df.shape[0]\n",
        "  missing_pct = round(missing/total*100,2)\n",
        "\n",
        "  # interpolate using the time index\n",
        "  if missing >0:\n",
        "    df.value.interpolate(method='time',inplace=True)\n",
        "    action = 'interpolate'\n",
        "  else:\n",
        "    action = 'none'\n",
        "\n",
        "  # Print the results\n",
        "  if maturity is not None:\n",
        "    summary = ['Economic Indicator',funct+':'+maturity,str(total),str(missing),str(missing_pct)+'%',action]\n",
        "  else:\n",
        "    summary = ['Economic Indicator',funct,str(total),str(missing),str(missing_pct)+'%',action]\n",
        "\n",
        "\n",
        "  return {'summary':summary,'data':df}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYLKNk44ymPX"
      },
      "source": [
        "def get_technical_indicators(symbol,funct,key,interval,time_period=None,throttle=0):\n",
        "  \"\"\"\n",
        "  Returns Technical Indicators (only works for stocks, not cyrpto)\n",
        "  MACD:   symbol,interval\n",
        "  RSI:    symbol,interval,time_period\n",
        "  BBANDS: symbol,interval,time_period\n",
        "\n",
        "  Parameters:\n",
        "          interval: (1min, 5min, 15min, 30min, 60min, daily, weekly, monthly)\n",
        "          series_type: (open, close,high,low)-default to close\n",
        "          timer_periods: Integer\n",
        "  \"\"\"\n",
        "  # build the query string\n",
        "  if funct =='MACD':\n",
        "    url = f'https://www.alphavantage.co/query?function={funct}&symbol={symbol}&interval={interval}&series_type=close&apikey={key}'\n",
        "  if funct in ['RSI','BBANDS']:\n",
        "    url = f'https://www.alphavantage.co/query?function={funct}&symbol={symbol}&interval={interval}&series_type=close&time_period={time_period}&apikey={key}'\n",
        "\n",
        "  # request data as json, convert to dict, pause request to avoid the data throttle\n",
        "  r = requests.get(url)\n",
        "  time.sleep(throttle)\n",
        "  d = r.json()\n",
        "\n",
        "  # extract to a df, add the indicator name, convert the index to datetime\n",
        "  df = pd.DataFrame(d[f'Technical Analysis: {funct}']).T\n",
        "  df.index = pd.to_datetime(df.index)\n",
        "\n",
        "  # convert the data to float\n",
        "  for col in df.columns:\n",
        "    df[col] = df[col].astype('float')\n",
        "\n",
        "  # check for missing data\n",
        "  missing = df.isnull().any().sum()\n",
        "  total = len(df)\n",
        "  missing_pct = round(missing/total*100,2)\n",
        "\n",
        "\n",
        "  # Print the results\n",
        "  summary=['Technical Indicator',funct,str(total),str(missing),str(missing_pct)+'%','none']\n",
        "\n",
        "  return {'summary':summary,'data':df}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiDgCjCThWo7"
      },
      "source": [
        "def get_crypto_data(symbol,key):\n",
        "  \"\"\"\n",
        "  Pulls daily crypto prices from alpha advantage.\n",
        "  Inputs:\n",
        "    symbol: ETH, BTC, DOGE\n",
        "    key:    The alpha advantage API key\n",
        "  Output:\n",
        "    a dataframe of crypto prices: open,high,low, close, volume\n",
        "  \"\"\"\n",
        "  # build query string, get data as json and convert to a dict\n",
        "  url = f'https://www.alphavantage.co/query?function=DIGITAL_CURRENCY_DAILY&symbol={symbol}&market=CAD&apikey={key}'\n",
        "  r = requests.get(url)\n",
        "  d = r.json()\n",
        "\n",
        "  # extract data to df\n",
        "  df=pd.DataFrame(d['Time Series (Digital Currency Daily)']).T\n",
        "\n",
        "  # remove columns not required\n",
        "  # returns the price in two currencies, just keep USD\n",
        "  cols = [c for c in df.columns if '(CAD)' not in c]\n",
        "  df=df.loc[:, cols]\n",
        "  df.columns = ['open','high','low','close','volume','marketcap']\n",
        "  df.drop(['marketcap'],axis=1,inplace=True)\n",
        "\n",
        "  # change data types\n",
        "  df.index = pd.to_datetime(df.index)\n",
        "\n",
        "  # convert datatype to float\n",
        "  for col in ['open','high','low','close','volume']:\n",
        "    df[col] = df[col].astype('float')\n",
        "\n",
        "  # add the cyrpto name\n",
        "  df['symbol'] = d['Meta Data']['3. Digital Currency Name']\n",
        "\n",
        "  return df"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-STBxxq1hm0X"
      },
      "source": [
        "def calc_bollinger(df,feature,window=20,st=2):\n",
        "  \"\"\"\n",
        "  Calculates bollinger bands for a price time-series.  Used for crypto currencies\n",
        "  Input: \n",
        "    df     : A dataframe of time-series prices\n",
        "    feature: The name of the feature in the df to calculate the bands for\n",
        "    window : The size of the rolling window.  Defaults to 20 days with is standard\n",
        "    st     : The number of standard deviations to use in the calculation. 2 is standard \n",
        "  Output: \n",
        "    Returns the df with the bollinger band columns added\n",
        "  \"\"\"\n",
        "\n",
        "  # rolling mean and stdev\n",
        "  rolling_m  = df[feature].rolling(window).mean()\n",
        "  rolling_st = df[feature].rolling(window).std()\n",
        "\n",
        "  # add the upper/lower and middle bollinger bands\n",
        "  df['b-upper']  = rolling_m + (rolling_st * st)\n",
        "  df['b-middle'] = rolling_m \n",
        "  df['b-lower']  = rolling_m - (rolling_st * st)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "376fLJt_hney"
      },
      "source": [
        "def calc_rsi(df,feature='close',window=14):\n",
        "  \"\"\"\n",
        "  Calculates the RSI for the input feature\n",
        "  Input:\n",
        "    df      : A dataframe with a time-series of prices\n",
        "    feature : The name of the feature in the df to calculate the bands for\n",
        "    window  : The size of the rolling window.  Defaults to 14 days which is standard\n",
        "  Output: \n",
        "    Returns the df with the rsi band column added\n",
        "  \"\"\"\n",
        "  # RSI\n",
        "  # calc the diff in daily prices, exclude nan\n",
        "  diff =df[feature].diff()\n",
        "  diff.dropna(how='any',inplace=True)\n",
        "\n",
        "  # separate positive and negitive changes\n",
        "  pos_m, neg_m = diff.copy(),diff.copy()\n",
        "  pos_m[pos_m<0]=0\n",
        "  neg_m[neg_m>0]=0\n",
        "\n",
        "  # positive/negative rolling means\n",
        "  prm = pos_m.rolling(window).mean()\n",
        "  nrm = neg_m.abs().rolling(window).mean()\n",
        "\n",
        "  # calc the rsi and add to the df\n",
        "  ratio = prm /nrm\n",
        "  rsi = 100.0 - (100.0 / (1.0 + ratio))\n",
        "  df['rsi']=rsi"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wa0NNXnghpWC"
      },
      "source": [
        "def calc_macd(df,feature='close'):\n",
        "  \"\"\"\n",
        "  Calculates the MACD and signial for the input feature\n",
        "  Input:\n",
        "    df      : A dataframe with a time-series of prices\n",
        "    feature : The name of the feature in the df to calculate the bands for\n",
        "  Output: \n",
        "    Returns the df with the macd columns added\n",
        "  \"\"\"\n",
        "  ema12 = df[feature].ewm(span=12,adjust=False).mean()\n",
        "  ema26 = df[feature].ewm(span=26,adjust=False).mean()\n",
        "  df['macd']=ema12-ema26\n",
        "  df['macd_signal'] = df['macd'].ewm(span=9,adjust=False).mean()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWSP1zOWiMPf"
      },
      "source": [
        "def get_ticker_data(symbol,key,outputsize='compact',throttle=0):\n",
        "  \"\"\"\n",
        "  Returns daily data for a stock (symbol)\n",
        "    outputsize: compact(last 100) or full (20 years)\n",
        "    key: apikey\n",
        "    symbols: OILK (oil ETF),BAR(gold ETF),VXZ (volatility ETF)\n",
        "  \"\"\"\n",
        "  url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={symbol}&outputsize={outputsize}&apikey={key}'\n",
        "  r = requests.get(url)\n",
        "  time.sleep(throttle)\n",
        "  d = r.json()\n",
        "\n",
        "  # extract data to a df\n",
        "  df = pd.DataFrame(d['Time Series (Daily)']).T\n",
        "  df.columns = ['open','high','low','close','volume']\n",
        "  df['symbol'] = d['Meta Data']['2. Symbol']\n",
        "\n",
        "  # change data types\n",
        "  df.index = pd.to_datetime(df.index)\n",
        "\n",
        "  # convert datatype to float\n",
        "  for col in ['open','high','low','close','volume']:\n",
        "    df[col] = df[col].astype('float')\n",
        "\n",
        "  # Calculate missing data\n",
        "  missing = sum(df.close.isna())\n",
        "  total = df.shape[0]\n",
        "  missing_pct = round(missing/total*100,2)\n",
        "\n",
        "  # Print the results\n",
        "  summary = ['Ticker',symbol,str(total),str(missing),str(missing_pct)+'%','none']\n",
        "\n",
        "  return {'summary':summary,'data':df}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5Pu1e7YvL73"
      },
      "source": [
        "def get_consolidated_stock_data(symbol,key,config,outputsize='compact',throttle=30,dropna=True):\n",
        "  \"\"\"\n",
        "  Pulls data from alpha advantage and consolidates\n",
        "  API Limitations: 5 API requests per minute and 500 requests per day\n",
        "  Inputs:\n",
        "    symbol: stock ticker\n",
        "    key   : api key\n",
        "    config: dictionary which lists the economic, technical and commodities to pull\n",
        "    outputsize: compact(latest 100) or full (up to 20 years of daily data)\n",
        "    throttle: number of seconds to wait between api requests\n",
        "    dropna: True/False, drops any records with nan\n",
        "  Output:\n",
        "    A dataframe with consolidated price data for the symbol + economic/technical\n",
        "    indicators and commodity prices\n",
        "  \"\"\"\n",
        "\n",
        "  # Result header and accumulator\n",
        "  header = ['Type','Data','Total','Missing',' % ','Action']\n",
        "  summary =[]\n",
        "\n",
        "  # Get stock prices\n",
        "  try:\n",
        "    results  = get_ticker_data(symbol,key,outputsize,0)\n",
        "    dff = results['data']\n",
        "    summary.append(results['summary'])\n",
        "    print(f'Complete:===>Ticker:{symbol}')\n",
        "  except:\n",
        "    print(f'Error:===>Ticker:{symbol}')\n",
        "\n",
        "\n",
        "\n",
        "  # Get Commodity prices\n",
        "  # ****************************************************************************\n",
        "  for commodity in config['Commodities']:\n",
        "    try:\n",
        "      # get prices\n",
        "      results = get_ticker_data(commodity,key,outputsize,throttle)\n",
        "      df = results['data']\n",
        "      summary.append(results['summary'])\n",
        "      print(f'Complete:===>Commodity:{commodity}')\n",
        "\n",
        "\n",
        "      # rename close to commodity name, remove unneeded columns and join with \n",
        "      # the stock prices by date\n",
        "      df.rename(columns={'close':commodity},inplace=True)\n",
        "      df.drop(['open','high','low','volume','symbol'],axis=1,inplace=True)\n",
        "      dff = dff.join(df,how='left')\n",
        "    except:\n",
        "      print(f\"Error===>Commodity:{commodity}\")\n",
        "\n",
        "\n",
        "  # Economic Indicators\n",
        "  # ****************************************************************************\n",
        "  # loop through the config to pull the requested data\n",
        "  for indicator,values in config['Economic'].items():\n",
        "    if indicator == 'TREASURY_YIELD':\n",
        "      for tr in values:\n",
        "        try:\n",
        "          results = get_economic_indicators(indicator,key,interval=tr['interval'],maturity=tr['maturity'],throttle=throttle)\n",
        "          summary.append(results['summary'])\n",
        "          print(f\"Complete:===>{indicator}:{tr['maturity']}\")\n",
        "\n",
        "          df = results['data']\n",
        "          dff = dff.join(df,how='left')\n",
        "          dff.rename(columns={\"value\": tr['name']},inplace=True)\n",
        "          dff.drop(['name', 'interval'], axis=1,inplace = True)\n",
        "        except:\n",
        "          print(f\"Error===>{indicator}:{tr['maturity']}\")\n",
        "   \n",
        "    else: \n",
        "      # daily\n",
        "      if values['interval']=='daily':\n",
        "        try:\n",
        "          results = get_economic_indicators(indicator,key,interval=values['interval'],throttle=throttle)\n",
        "          df = results['data']\n",
        "          summary.append(results['summary'])\n",
        "          print(f\"Complete:===>{indicator}\")\n",
        "\n",
        "          dff = dff.join(df,how='left')\n",
        "          dff.rename(columns={\"value\": values['name']},inplace=True)\n",
        "          dff.drop(['name', 'interval'], axis=1,inplace = True)\n",
        "        except:\n",
        "          print(f\"Error===>{indicator}\")\n",
        "  \n",
        "      else: \n",
        "        try:\n",
        "          # monthly or weekly\n",
        "          results = get_economic_indicators(indicator,key,throttle=throttle)\n",
        "          summary.append(results['summary'])\n",
        "          df = results['data']\n",
        "          print(f\"Complete:===>{indicator}\")\n",
        "\n",
        "          # reindex to daily, fill missing values forward\n",
        "          days = pd.date_range(start = min(df.index),end =max(df.index),freq='D')\n",
        "          df =df.reindex(days,method = 'ffill')\n",
        "      \n",
        "          # join with the other data\n",
        "          dff = dff.join(df,how='left')\n",
        "          dff.rename(columns={\"value\": values['name']},inplace=True)\n",
        "          dff.drop(['name', 'interval'], axis=1,inplace = True)\n",
        "        except:\n",
        "          print(f\"Error===>{indicator}\")\n",
        "\n",
        "  # # Technical Indicators\n",
        "  # ****************************************************************************\n",
        "  for indicator,values in config['Technical'].items():\n",
        "    try:\n",
        "      results = get_technical_indicators(symbol,indicator,key,values['interval'],values['time_period'],throttle)\n",
        "      df = results['data']\n",
        "      summary.append(results['summary'])\n",
        "\n",
        "      dff = dff.join(df,how='left')\n",
        "      print(f\"Complete:===>{indicator}\")\n",
        "    except:\n",
        "      print(f\"Error===>{indicator}\")\n",
        "\n",
        "  \n",
        "  # clean column names\n",
        "  dff.rename(columns={\"Real Upper Band\":'b-upper',\n",
        "                      \"Real Lower Band\":'b-lower',\n",
        "                      \"Real Middle Band\":\"b-middle\",\n",
        "                      \"RSI\":\"rsi\",\n",
        "                      \"MACD_Hist\":\"macd_hist\",\n",
        "                      \"MACD_Signal\":\"macd_signal\",\n",
        "                      \"MACD\":\"macd\"\n",
        "                      },inplace=True)\n",
        "      \n",
        "\n",
        "  # Fill in any missing data after joining all datasets\n",
        "  dff.fillna(method='bfill',inplace=True,axis = 0)\n",
        "\n",
        "  # drop rows with missing commodity prices\n",
        "  if dropna:\n",
        "    dff.dropna(how='any',inplace=True)\n",
        "\n",
        "  # print the results table\n",
        "  print(\"\\n\\n\")\n",
        "  print(tabulate(summary,header))\n",
        "\n",
        "  return dff"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpqdl7ZThrni"
      },
      "source": [
        "def get_consolidated_crypto_data(symbol,key,config,boll_window=20,boll_std=2,rsi_window=14,throttle=30,dropna=True):\n",
        "  \"\"\"\n",
        "  Pulls data from alpha advantage and consolidates\n",
        "  API Limitations: 5 API requests per minute and 500 requests per day\n",
        "  Inputs:\n",
        "    symbol: crypto ticker\n",
        "    key   : api key\n",
        "    config: dictionary which lists the economic indicators and commodities to pull\n",
        "    throttle: number of seconds to wait between api requests\n",
        "    dropna: True/False, drops any records with nan\n",
        "  Output:\n",
        "    A dataframe with consolidated price data for the symbol + economic/technical\n",
        "    indicators and commodity prices\n",
        "  \"\"\"\n",
        "\n",
        "  # Result header and accumulator\n",
        "  header = ['Type','Data','Total','Missing',' % ','Action']\n",
        "  summary =[]\n",
        "\n",
        "  # Get crypto prices\n",
        "  try:\n",
        "    dff  = get_crypto_data(symbol,key)\n",
        "    \n",
        "    # add month feature\n",
        "    dff['month'] = dff.index.month\n",
        "\n",
        "    print(f'Complete:===>Crypto:{symbol}')\n",
        "\n",
        "  except:\n",
        "    print(f'Error:===>Crypto:{symbol}')\n",
        "\n",
        "\n",
        "  # Get Commodity prices\n",
        "  # ****************************************************************************\n",
        "  for commodity in config['Commodities']:\n",
        "    try:\n",
        "      # get prices\n",
        "      results = get_ticker_data(commodity,key,'full',throttle)\n",
        "      df = results['data']\n",
        "      summary.append(results['summary'])\n",
        "      print(f'Complete:===>Commodity:{commodity}')\n",
        "\n",
        "\n",
        "      # rename close to commodity name, remove unneeded columns and join with \n",
        "      # the stock prices by date\n",
        "      df.rename(columns={'close':commodity},inplace=True)\n",
        "      df.drop(['open','high','low','volume','symbol'],axis=1,inplace=True)\n",
        "      dff = dff.join(df,how='left')\n",
        "    except:\n",
        "      print(f\"Error===>Commodity:{commodity}\")\n",
        "\n",
        "\n",
        "  # Economic Indicators\n",
        "  # ****************************************************************************\n",
        "  # loop through the config to pull the requested data\n",
        "  for indicator,values in config['Economic'].items():\n",
        "    if indicator == 'TREASURY_YIELD':\n",
        "      for tr in values:\n",
        "        try:\n",
        "          results = get_economic_indicators(indicator,key,interval=tr['interval'],maturity=tr['maturity'],throttle=throttle)\n",
        "          summary.append(results['summary'])\n",
        "          print(f\"Complete:===>{indicator}:{tr['maturity']}\")\n",
        "\n",
        "          df = results['data']\n",
        "          dff = dff.join(df,how='left')\n",
        "          dff.rename(columns={\"value\": tr['name']},inplace=True)\n",
        "          dff.drop(['name', 'interval'], axis=1,inplace = True)\n",
        "        except:\n",
        "          print(f\"Error===>{indicator}:{tr['maturity']}\")\n",
        "   \n",
        "    else: \n",
        "      # daily\n",
        "      if values['interval']=='daily':\n",
        "        try:\n",
        "      \n",
        "          results = get_economic_indicators(indicator,key,interval=values['interval'],throttle=throttle)\n",
        "          df = results['data']\n",
        "          summary.append(results['summary'])\n",
        "          print(f\"Complete:===>{indicator}\")\n",
        "\n",
        "          dff = dff.join(df,how='left')\n",
        "          dff.rename(columns={\"value\": values['name']},inplace=True)\n",
        "          dff.drop(['name', 'interval'], axis=1,inplace = True)\n",
        "        except:\n",
        "          print(f\"Error===>{indicator}\")\n",
        "  \n",
        "      else: \n",
        "        try:\n",
        "          # monthly or weekly\n",
        "         \n",
        "          results = get_economic_indicators(indicator,key,throttle=throttle)\n",
        "          summary.append(results['summary'])\n",
        "          df = results['data']\n",
        "          print(f\"Complete:===>{indicator}\")\n",
        "\n",
        "          # reindex to daily, fill missing values forward\n",
        "          days = pd.date_range(start = min(df.index),end =max(df.index),freq='D')\n",
        "          df =df.reindex(days,method = 'ffill')\n",
        "      \n",
        "          # join with the other data\n",
        "          dff = dff.join(df,how='left')\n",
        "          dff.rename(columns={\"value\": values['name']},inplace=True)\n",
        "          dff.drop(['name', 'interval'], axis=1,inplace = True)\n",
        "        except:\n",
        "          print(f\"Error===>{indicator}\")\n",
        "\n",
        "  # # Technical Indicators\n",
        "  # ****************************************************************************\n",
        "  calc_rsi(dff,'close',rsi_window)\n",
        "  calc_bollinger(dff,'close',boll_window,boll_std)\n",
        "  calc_macd(dff,'close')\n",
        "      \n",
        "\n",
        "  # Fill in any missing data after joining all datasets\n",
        "  dff.fillna(method='bfill',inplace=True,axis = 0)\n",
        "\n",
        "  # drop rows with missing commodity prices\n",
        "  if dropna:\n",
        "    dff.dropna(how='any',inplace=True)\n",
        "\n",
        "  # print the results table\n",
        "  print(\"\\n\\n\")\n",
        "  print(tabulate(summary,header))\n",
        "\n",
        "  return dff"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoD6RJ-zoVhg"
      },
      "source": [
        "def transform_stationary(df,features_to_transform,transform='log'):\n",
        "  \"\"\"\n",
        "  Transform time-series data using a log or boxcox transform.  Calculate the augmented\n",
        "  dickey-fuller (ADF) test for stationarity after the transform\n",
        "  Inputs:\n",
        "    df: a dataframe of features\n",
        "    features_to_transform: A list of features to apply the transform\n",
        "    transform: The transform to apply (log, boxbox)\n",
        "  Output\n",
        "    Applies the transforms inplace in df\n",
        "  \"\"\"\n",
        "  # transform each column in the features_to_transform list\n",
        "  for feature in df.columns:\n",
        "    if feature in features_to_transform:\n",
        "      # log transform\n",
        "      if transform=='log':\n",
        "        df[feature] = df[feature].apply(np.log)\n",
        "\n",
        "      # boxcox transform  \n",
        "      elif transform=='boxcox':\n",
        "        bc,_ = stats.boxcox(df[feature])\n",
        "        df[feature] = bc\n",
        "\n",
        "      else:\n",
        "        print(\"Transformation not recognized\")\n",
        "\n",
        "  # check the closing price for stationarity using the augmented dicky fuller test\n",
        "  t_stat, p_value, _, _, critical_values, _  = adfuller(df.close.values, autolag='AIC')\n",
        "  print('\\n\\nAugmented Dicky Fuller Test for Stationarity')\n",
        "  print(\"=\"*60)\n",
        "  print(f'ADF Statistic: {t_stat:.2f}')\n",
        "  for key, value in critical_values.items():\n",
        "    print('Critial Values:')\n",
        "    if t_stat < value:\n",
        "      print(f'   {key}, {value:.2f} => non-stationary')\n",
        "    else:\n",
        "      print(f'   {key}, {value:.2f} => stationary')\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDeroa2syZkV"
      },
      "source": [
        "def shift_features(df,features_shift):\n",
        "  \"\"\"\n",
        "  Shifts features by time periods to convert them to lagged indicators\n",
        "  Input:\n",
        "    df: dataframe of features\n",
        "    features_shift: dictionary  of {feature:period shift}\n",
        "  Output:\n",
        "    df: original dataframe + the shifted features\n",
        "  \"\"\"\n",
        "  dff = df.copy()\n",
        "  for feature,shift in features_shift.items():\n",
        "    t_shift = pd.DataFrame(dff[feature].shift(periods=shift))\n",
        "    dff =dff.join(t_shift,how='left',rsuffix='_shift')\n",
        "\n",
        "  # remove nan introducted with lag features\n",
        "  dff.dropna(how='any',inplace=True)\n",
        "\n",
        "  return dff"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrS1WDkvpvrj"
      },
      "source": [
        "def prepare_data(df,n_steps,features=[]):\n",
        "  \"\"\"\n",
        "  Filter, scale and convert dataframe data to numpy arrays\n",
        "\n",
        "  Inputs: \n",
        "    df       => A dataframe of observations with features and y-labels\n",
        "    y        => The name of the column that is the truth labels\n",
        "    features => A list of features.  Used to subset columns\n",
        "\n",
        "  Outputs:\n",
        "    scaled_y => numpy array of the y-label data\n",
        "    scaled_x => numpy array of the training features\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # subset the latest n_steps rows to be used for prediction\n",
        "  df = df.iloc[0:n_steps,:]\n",
        "\n",
        "  # reverse the index such that dates are in chronological order\n",
        "  df = df.iloc[::-1]\n",
        "\n",
        "  # Subset features, get the y-label values\n",
        "  df_y = df['close']\n",
        "  df_X = df[features]\n",
        "\n",
        "  # replace the date index with an integer index\n",
        "  idx_dates = df.index\n",
        "  df_X.reset_index(drop=True,inplace=True)\n",
        "\n",
        "  # convert to numpay arrays\n",
        "  array_X = np.array(df_X)\n",
        "  array_y = np.array(df_y).reshape(-1,1)\n",
        "\n",
        "\n",
        "  # print the output\n",
        "  print(\"\\nData Preparation\")\n",
        "  print(\"=\"*60)\n",
        "  print(f\"=> {len(features)} Features\")\n",
        "  print(f\"=> Input Dimensions :{array_X.shape}\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "  return idx_dates, array_y,array_X"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9gSM-QNIlec"
      },
      "source": [
        "def make_predictions(model,scaler,scaled_X,n_steps,n_features,n_pred,start_date):\n",
        "  \"\"\"\n",
        "  Predict the next n_pred days with n_steps of daily data\n",
        "  Input:\n",
        "    model: A trained LSTM model\n",
        "    scaler: The scaler used\n",
        "    scaled_X: scaled input features\n",
        "    n_steps: the number of input days used in the model\n",
        "    n_features: the number of features used in the model\n",
        "    n_pred: the number of days predicted in the model\n",
        "    start_date: the start date of the prediction window\n",
        "  Output:\n",
        "    a data frame of predicted prices\n",
        "  \"\"\"\n",
        "\n",
        "  # Predict the prices\n",
        "  y_pred_scaled = model.predict(scaled_X.reshape(1,n_steps,n_features))\n",
        "\n",
        "  # convert units back to the original scale\n",
        "  y_pred_unscaled = scaler.inverse_transform(y_pred_scaled)\n",
        "\n",
        "  # convert from log transform back to original scale\n",
        "  y_pred_np = np.exp(y_pred_unscaled)\n",
        "\n",
        "  # set the date index\n",
        "  pred_dates = pd.date_range(start_date + datetime.timedelta(days=1), periods=n_pred,freq='D').tolist()\n",
        "\n",
        "\n",
        "  # convert to dataframe\n",
        "  df_pred = pd.DataFrame(y_pred_np.T,columns=['pred'])\n",
        "  df_pred['actual']= np.nan\n",
        "  df_pred['date'] = pred_dates\n",
        "  df_pred.set_index(['date'],inplace=True)\n",
        "  df_pred.index = pd.to_datetime(df_pred.index)\n",
        "  df_pred =df_pred[['actual','pred']]\n",
        "\n",
        "  return df_pred"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCubo_Lxt0uu"
      },
      "source": [
        "def roll_predictions(df_new,df_pred=None,df_hist =None):\n",
        "  \"\"\"\n",
        "  Updates previous predicted prices with actual prices, and adds\n",
        "  the next n_pred prediction window\n",
        "  Input:\n",
        "    df_new: The new dataset of inputs\n",
        "    df_pred: A dataframe of predicted prices\n",
        "    df_hist: A dataframe that stores the actual/predicted prices \n",
        "            (the output of this function)\n",
        "  Output:\n",
        "    A dataframe of up to date prices with the next prediction window.\n",
        "    Incluces the daily and cumulative prediction error\n",
        "  \"\"\"\n",
        "  # First time creating history file\n",
        "  if df_pred is None and df_hist is None:\n",
        "    # create the initial history of prices (without predictions)\n",
        "    df_hist_new = pd.DataFrame(df_new['close'],columns=['close'])\n",
        "    df_hist_new.columns = ['actual']\n",
        "    df_hist_new['pred']=np.nan\n",
        "    df_hist_new['diff']=np.nan\n",
        "    df_hist_new['diff_cum']=np.nan\n",
        "    df_hist_new['actual_pct']=np.nan\n",
        "    df_hist_new['pred_pct']=np.nan\n",
        "\n",
        "\n",
        "  else:\n",
        "    # append to existing history file\n",
        "    # make a copy of df_hist\n",
        "    df = df_hist.copy()\n",
        "\n",
        "    # Get yesterdays closing price\n",
        "    yesterday = df_new.index.max()\n",
        "    yesterdays_close = df_new.loc[yesterday,'close'].item()\n",
        "\n",
        "    # update df_hist with yesterdays_close,\n",
        "    update_price(df,yesterday,yesterdays_close,'actual')\n",
        "\n",
        "\n",
        "    # remove old predictions \n",
        "    # yesterdays nan should have been replaced in the prevous step\n",
        "    df = df[~df['actual'].isnull()]\n",
        "\n",
        "    # add new predictions\n",
        "    df_hist_new =pd.concat([df,df_pred])\n",
        "\n",
        "    # calculate the difference between actual/predicted values\n",
        "    # for current period and cumulative\n",
        "    df_hist_new['diff'] = df_hist_new['pred']-df_hist_new['actual']\n",
        "    df_hist_new['diff_cum'] = df_hist_new['diff'].cumsum()\n",
        "\n",
        "    # calculate the percent change in actual / predicted values\n",
        "    df_hist_new['actual_pct'] = df_hist_new['actual'].pct_change() * 100\n",
        "    df_hist_new['pred_pct'] = df_hist_new['pred'].pct_change() * 100\n",
        "\n",
        "    #sort by date\n",
        "    df_hist_new.sort_index(inplace=True)\n",
        "\n",
        "  return df_hist_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM-KsAG3IVxJ"
      },
      "source": [
        "# def roll_predictions2(df_new,df_pred=None,df_hist =None):\n",
        "#   \"\"\"\n",
        "#   Updates previous predicted prices with actual prices, and adds\n",
        "#   the next n_pred prediction window\n",
        "#   Input:\n",
        "#     df_new: The new dataset of inputs\n",
        "#     df_pred: A dataframe of predicted prices\n",
        "#     df_hist: A dataframe that stores the actual/predicted prices \n",
        "#             (the output of this function)\n",
        "#   Output:\n",
        "#     A dataframe of up to date prices with the next prediction window.\n",
        "#     Incluces the daily and cumulative prediction error\n",
        "#   \"\"\"\n",
        "#   # reverse the order of the dataframe\n",
        "#   df_new = df_new.iloc[::-1]\n",
        "\n",
        "\n",
        "#   # First time creating history file\n",
        "#   if df_pred is None and df_hist is None:\n",
        "#     # create the initial history of prices (without predictions)\n",
        "#     df_hist_new = pd.DataFrame(df_new['close'],columns=['close'])\n",
        "#     df_hist_new.columns = ['actual']\n",
        "#     df_hist_new['pred']=np.nan\n",
        "#     df_hist_new['diff']=np.nan\n",
        "#     df_hist_new['diff_cum']=np.nan\n",
        "#     df_hist_new['actual_pct']=np.nan\n",
        "#     df_hist_new['diff_pct']=np.nan\n",
        "#     df_hist_new['naive']=np.nan\n",
        "#     df_hist_new['naive_diff']=np.nan\n",
        "#     df_hist_new['naive_diff_cum']=np.nan\n",
        "#     df_hist_new['naive_diff_pct']=np.nan\n",
        "\n",
        "#   else:\n",
        "#     # append to existing history file\n",
        "#     # make a copy of df_hist\n",
        "#     df = df_hist.copy()\n",
        "\n",
        "#     # Get yesterdays closing price\n",
        "#     yesterday = df_new.index.max()\n",
        "#     yesterdays_close = df_new.loc[yesterday,'close'].item()\n",
        "#     naive_close = df_new.loc[yesterday - datetime.timedelta(days=1),'close'].item()\n",
        "\n",
        "#     # update df_hist with yesterdays_close,\n",
        "#     update_price(df,yesterday,yesterdays_close,'actual')\n",
        "\n",
        "#     # update df_hist naive prediction with yesterays close\n",
        "#     update_price(df,df_new.index.max(),naive_close,'naive')\n",
        "\n",
        "\n",
        "#     # remove old predictions \n",
        "#     # yesterdays nan should have been replaced in the prevous step\n",
        "#     df = df[~df['actual'].isnull()]\n",
        "\n",
        "#     # add new predictions\n",
        "#     df_hist_new =pd.concat([df,df_pred])\n",
        "\n",
        "#     # calculate the difference between actual/predicted values\n",
        "#     # for current period and cumulative\n",
        "#     df_hist_new['diff'] = df_hist_new['pred']-df_hist_new['actual']\n",
        "#     df_hist_new['diff_cum'] = df_hist_new['diff'].cumsum()\n",
        "#     df_hist_new['naive_diff'] = df_hist_new['naive']-df_hist_new['actual']\n",
        "#     df_hist_new['naive_diff_cum'] = df_hist_new['naive_diff'].cumsum()\n",
        "\n",
        "#     # calculate the percent change in actual / predicted values\n",
        "#     df_hist_new['actual_pct'] = df_hist_new['actual'].pct_change() * 100\n",
        "#     df_hist_new['diff_pct'] = df_hist_new['diff'] / df_hist_new['actual'] *100\n",
        "#     df_hist_new['naive_diff_pct'] = df_hist_new['naive_diff'] / df_hist_new['actual']*100\n",
        "\n",
        "\n",
        "#     #sort by date\n",
        "#     df_hist_new.sort_index(inplace=True)\n",
        "\n",
        "#   return df_hist_new"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw_dYMLiNXEK"
      },
      "source": [
        "def update_price(df_hist,date,value,type='actual'):\n",
        "  \"\"\"\n",
        "  Updates the price in the history dataframe\n",
        "  Input: \n",
        "    df_hist: the dataframe that contains the history of prices/predicitons/metrics\n",
        "    date: the date to update\n",
        "    value: the price to update\n",
        "    type: actual or predicted price to update\n",
        "  \"\"\"\n",
        "  # update the price as of the date\n",
        "  # should be yesterdays price\n",
        "  df_hist.at[date,type]=value\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKfA5xwsRq0f"
      },
      "source": [
        "def plot_actual_predicted(name,df):\n",
        "  \"\"\"\n",
        "  Plots the prices as a time-series showing actual/predicted values with daily and \n",
        "  cumulative prediction errors\n",
        "  Input:\n",
        "    name: the name of the stock/crypto\n",
        "    df: the historical dataframe (output from roll_predictions)\n",
        "  \"\"\"\n",
        "  fig = make_subplots(rows=3, \n",
        "                      cols=1,\n",
        "                      shared_xaxes=True,\n",
        "                      vertical_spacing=0.1,\n",
        "                      subplot_titles = ('Actual vs Predicted Closing Price','Daily Error','Cumulative Error'))\n",
        "  # Actual prices\n",
        "  fig.add_trace(go.Scatter(\n",
        "      x=df.index,\n",
        "      y=df.actual,\n",
        "      fill='tozeroy',\n",
        "      mode = 'lines',\n",
        "      line =dict(color=\"#ccc\"),\n",
        "      name = 'Actual'),\n",
        "      row=1,col=1\n",
        "  )\n",
        "  # predicted prices\n",
        "  fig.add_trace(go.Scatter(\n",
        "      x=df.index,\n",
        "      y=df.pred,\n",
        "      fill = 'tozeroy',\n",
        "      mode = 'lines+markers',\n",
        "      line = dict(color='rgba(247, 12, 55, 0.1)'),\n",
        "      marker =dict(size=5),\n",
        "      name= 'Predicted'),\n",
        "      row=1,col=1\n",
        "  )\n",
        "  # daily error\n",
        "  fig.add_trace(go.Bar(\n",
        "      x = df.index,\n",
        "      y = df['diff'],\n",
        "      name = 'Error',\n",
        "      marker_color = 'rgba(247, 12, 55, 0.5)'\n",
        "  ),row=2,col=1)\n",
        "\n",
        "  # cumulative error\n",
        "  fig.add_trace(go.Scatter( \n",
        "      x=df.index,\n",
        "      y=df['diff_cum'],\n",
        "      fill = 'tozeroy',\n",
        "      line = dict(color='rgba(247, 12, 55, 0.5)'),\n",
        "      mode = 'lines',\n",
        "      name = 'Cumulative Error-LSTM Model'\n",
        "  ),row=3,col=1)\n",
        "  \n",
        "  fig.update_layout(height=600, \n",
        "                    width=800, \n",
        "                    template = 'plotly_white',\n",
        "                    title_text=f\"{name}: Actual Vs. Predicted Prices\")\n",
        "  fig.show()\n"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJUd0nOtNeBq"
      },
      "source": [
        "# def plot_actual_predicted2(name,df):\n",
        "#   \"\"\"\n",
        "#   Plots the prices as a time-series showing actual/predicted values with daily and \n",
        "#   cumulative prediction errors\n",
        "#   Input:\n",
        "#     name: the name of the stock/crypto\n",
        "#     df: the historical dataframe (output from roll_predictions)\n",
        "#   \"\"\"\n",
        "#   fig = make_subplots(rows=3, \n",
        "#                       cols=1,\n",
        "#                       shared_xaxes=True,\n",
        "#                       vertical_spacing=0.1,\n",
        "#                       subplot_titles = ('Actual vs Predicted Closing Price','Daily Error','Cumulative Error'))\n",
        "#   # Actual prices\n",
        "#   fig.add_trace(go.Scatter(\n",
        "#       x=df.index,\n",
        "#       y=df.actual,\n",
        "#       fill='tozeroy',\n",
        "#       mode = 'lines',\n",
        "#       line =dict(color=\"#ccc\"),\n",
        "#       name = 'Actual'),\n",
        "#       row=1,col=1\n",
        "#   )\n",
        "#   # predicted prices\n",
        "#   fig.add_trace(go.Scatter(\n",
        "#       x=df.index,\n",
        "#       y=df.pred,\n",
        "#       fill = 'tozeroy',\n",
        "#       mode = 'lines+markers',\n",
        "#       line = dict(color='rgba(247, 12, 55, 0.1)'),\n",
        "#       marker =dict(size=5),\n",
        "#       name= 'Predicted'),\n",
        "#       row=1,col=1\n",
        "#   )\n",
        "#   # daily error\n",
        "#   fig.add_trace(go.Bar(\n",
        "#       x = df.index,\n",
        "#       y = df['diff'],\n",
        "#       name = 'Error',\n",
        "#       marker_color = 'rgba(247, 12, 55, 0.5)'\n",
        "#   ),row=2,col=1)\n",
        "\n",
        "#   # cumulative error\n",
        "#   fig.add_trace(go.Scatter( \n",
        "#       x=df.index,\n",
        "#       y=df['diff_cum'],\n",
        "#       fill = 'tozeroy',\n",
        "#       line = dict(color='rgba(247, 12, 55, 0.5)'),\n",
        "#       mode = 'lines',\n",
        "#       name = 'Cumulative Error-LSTM Model'\n",
        "#   ),row=3,col=1)\n",
        "  \n",
        "#   fig.add_trace(go.Scatter(\n",
        "#       x=df.index,\n",
        "#       y=df['naive_diff_cum'],\n",
        "#       fill = 'tozeroy',\n",
        "#       line = dict(color='rgba(53, 81, 92, 0.5)'),\n",
        "#       mode = 'lines',\n",
        "#       name = 'Cumulative Error- Naive Model'\n",
        "#   ),row=3,col=1)"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui1WaOPTc0sh"
      },
      "source": [
        "# Historical Back Test\n",
        "+ pull all data to current date\n",
        "+ start in jan 1,2021 and make the 5 day prediction\n",
        "+ roll the model forward 1 day, make the next 5 day prediction\n",
        "+ increment by 1 day, and repeat until the current date is reached"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jq4R5SgvOh2"
      },
      "source": [
        "## VM Ware\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXt3koEHqqSB"
      },
      "source": [
        "# initial hist file\n",
        "stock = 'VMW'\n",
        "features_to_transform = ['open','close','high','low']\n",
        "transform = 'log'\n",
        "n_steps = 40\n",
        "n_predict = 5\n",
        "config ={'Economic':\n",
        "         {'TREASURY_YIELD':[{'interval':'daily','maturity':'5year','name':'yield5y'},\n",
        "                            {'interval':'daily','maturity':'10year','name':'yield10y'},\n",
        "                            {'interval':'daily','maturity':'30year','name':'yield30y'},\n",
        "                            {'interval':'daily','maturity':'3month','name':'yield3m'}\n",
        "                            ],\n",
        "          'FEDERAL_FUNDS_RATE':{'interval':'daily','name':'ir'},\n",
        "          'NONFARM_PAYROLL':{'interval':'monthly','name':'nfp'},\n",
        "          'UNEMPLOYMENT':{'interval':'monthly','name':'unemployment'},\n",
        "          'CONSUMER_SENTIMENT':{'interval':'monthly','name':'cs'},\n",
        "          'INFLATION_EXPECTATION':{'interval':'monthly','name':'infl'},\n",
        "          },\n",
        "         'Technical':{\n",
        "           'BBANDS':{'interval':'daily','time_period':20},\n",
        "           'RSI':{'interval':'daily','time_period':14},\n",
        "           'MACD':{'interval':'daily','time_period':None}\n",
        "           },\n",
        "         'Commodities':['GLD','OIL','SPY','VXX','QQQ','SKYY','VGT']\n",
        "         }\n",
        "\n",
        "# get full data history\n",
        "df_new = get_consolidated_stock_data(stock,key,config,'full')\n",
        "\n",
        "\n",
        "# get the trained model\n",
        "model = keras.models.load_model(f'/content/drive/MyDrive/Colab Notebooks/models/model_VWM_final')\n",
        "\n",
        "# Get the features used in the final model\n",
        "df_vmw_features = pd.read_pickle(f'/content/drive/MyDrive/Colab Notebooks/data/{stock}_market_data.pickle')\n",
        "features = [f for f in df_vmw_features.columns if f not in ['symbol']]\n",
        "\n",
        "# # Backtest to Jan 1,2021\n",
        "start_date = '2021-1-1'\n",
        "\n",
        "# Create first df_hist file\n",
        "df_tmp = df_new.loc[df_new.index <=start_date]\n",
        "df_hist_new =roll_predictions(df_tmp)\n",
        "df_hist_new.to_pickle(f'/content/drive/MyDrive/Colab Notebooks/data/{stock}_hist.pickle')\n",
        "\n",
        "# backtest\n",
        "# datelist = pd.date_range(start_date, periods=266).tolist()\n",
        "datelist = pd.date_range(start='2021-01-01',end='2021-09-24',freq='B').tolist()\n",
        "for dt in datelist:\n",
        "\n",
        "  print(dt)\n",
        "  print(\"=\"*60)\n",
        "  df_tmp = df_new.loc[df_new.index <=dt]\n",
        "  df_orig = df_tmp.copy()\n",
        "\n",
        "  # transform\n",
        "  print(\"=> log transpose\\n\")\n",
        "  transform_stationary(df_tmp,features_to_transform,transform)\n",
        "\n",
        "  #prepare\n",
        "  print(\"=>prepare data\\n\")\n",
        "  idx_dates, array_y, array_X = prepare_data(df_tmp,n_steps,features)\n",
        "\n",
        "  # scale the input and outputs\n",
        "  print(\"=>scale data\\n\")\n",
        "  scaler_X = MinMaxScaler(feature_range=(0,1))\n",
        "  scaled_X = scaler_X.fit_transform(array_X)\n",
        "  scaler_y = MinMaxScaler(feature_range=(0,1))\n",
        "  scaled_y = scaler_y.fit_transform(array_y)\n",
        "\n",
        "\n",
        "  # make predictions\n",
        "  print(\"=>make predictions\\n\")\n",
        "  df_pred =make_predictions(model,scaler_y,scaled_X,n_steps,len(features),n_predict,df_tmp.index.max())\n",
        "\n",
        "  # get the previous df_hist data\n",
        "  print(\"=>get previous file\\n\")\n",
        "  df_hist = pd.read_pickle(f'/content/drive/MyDrive/Colab Notebooks/data/{stock}_hist.pickle')\n",
        "\n",
        "  # update the hist file with yesterdays close price, and add the new predictions\n",
        "  print(\"=>roll file forward\\n\")\n",
        "  df_hist_new =roll_predictions(df_orig,df_pred,df_hist)\n",
        "  df_hist_new.to_pickle(f'/content/drive/MyDrive/Colab Notebooks/data/{stock}_hist.pickle')\n",
        "\n",
        "\n",
        "# plot\n",
        "plot_actual_predicted(stock,df_hist_new)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMQ28vtIEcpV"
      },
      "source": [
        "## Bitcoin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7YlF_3sKGqg"
      },
      "source": [
        "stock = 'BTC'\n",
        "features_to_transform = ['open','close','high','low','b-upper','b-lower','b-middle']\n",
        "transform = 'log'\n",
        "n_steps = 40\n",
        "n_predict = 5\n",
        "\n",
        "config ={'Economic':\n",
        "         {'TREASURY_YIELD':[\n",
        "                            {'interval':'daily','maturity':'5year','name':'yield5y'},\n",
        "                            {'interval':'daily','maturity':'10year','name':'yield10y'}\n",
        "                            ],\n",
        "          },\n",
        "         'Commodities':['SPY']\n",
        "         }\n",
        "\n",
        "get full data history\n",
        "df_new = get_consolidated_crypto_data(stock,key)\n",
        "\n",
        "\n",
        "get the trained model\n",
        "model = keras.models.load_model(f'/content/drive/MyDrive/Colab Notebooks/models/model_BTC_log_features_optimized')\n",
        "\n",
        "# Get the features used in the final model\n",
        "df_btc_features = pd.read_pickle(f'/content/drive/MyDrive/Colab Notebooks/data/{stock}_market_data_features.pickle')\n",
        "features = [f for f in df_btc_features.columns if f not in ['symbol']]\n",
        "\n",
        "\n",
        "# initial hist file\n",
        "start_date = '2021-01-01'\n",
        "df_tmp = df_new.loc[df_new.index <=start_date]\n",
        "df_hist_new =roll_predictions(df_tmp)\n",
        "df_hist_new.to_pickle(f'/content/drive/MyDrive/Colab Notebooks/data/{stock}_hist.pickle')\n",
        "\n",
        "# backtest for sep 2021\n",
        "datelist = pd.date_range(start_date, periods=270).tolist()\n",
        "for dt in datelist:\n",
        "\n",
        "  print(dt)\n",
        "  print(\"=\"*60)\n",
        "  df_tmp = df_new.loc[df_new.index <=dt]\n",
        "  df_orig = df_tmp.copy()\n",
        "\n",
        "  # transform\n",
        "  print(\"=> log transpose\\n\")\n",
        "  transform_stationary(df_tmp,features_to_transform,transform)\n",
        "\n",
        "  #prepare\n",
        "  print(\"=>prepare data\\n\")\n",
        "  idx_dates, array_y, array_X = prepare_data(df_tmp,n_steps,features)\n",
        "\n",
        "  # scale the input and outputs\n",
        "  print(\"=>scale data\\n\")\n",
        "  scaler_X = MinMaxScaler(feature_range=(0,1))\n",
        "  scaled_X = scaler_X.fit_transform(array_X)\n",
        "  scaler_y = MinMaxScaler(feature_range=(0,1))\n",
        "  scaled_y = scaler_y.fit_transform(array_y)\n",
        "\n",
        "\n",
        "  # make predictions\n",
        "  print(\"=>make predictions\\n\")\n",
        "  # df_pred =make_predictions(model,scaler_y,scaled_X,n_steps,len(features),n_predict,df_tmp.index.max()-datetime.timedelta(days=1))\n",
        "  df_pred =make_predictions(model,scaler_y,scaled_X,n_steps,len(features),n_predict,df_tmp.index.max())\n",
        "\n",
        "  # get the previous df_hist data\n",
        "  print(\"=>get previous file\\n\")\n",
        "  df_hist = pd.read_pickle(f'/content/drive/MyDrive/Colab Notebooks/data/{stock}_hist.pickle')\n",
        "\n",
        "  # update the hist file with yesterdays close price, and add the new predictions\n",
        "  print(\"=>roll file forward\\n\")\n",
        "  df_hist_new =roll_predictions(df_orig,df_pred,df_hist)\n",
        "  df_hist_new.to_pickle(f'/content/drive/MyDrive/Colab Notebooks/data/{stock}_hist.pickle')\n",
        "\n",
        "\n",
        "# plot\n",
        "plot_actual_predicted(stock,df_hist_new)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDo5QHifFICf"
      },
      "source": [
        "# BTC- with naive model\n",
        "# stock = 'BTC'\n",
        "# features_to_transform = ['open','close','high','low','b-upper','b-lower','b-middle']\n",
        "# transform = 'log'\n",
        "# n_steps = 40\n",
        "# n_predict = 5\n",
        "\n",
        "# config ={'Economic':\n",
        "#          {'TREASURY_YIELD':[\n",
        "#                             {'interval':'daily','maturity':'5year','name':'yield5y'},\n",
        "#                             {'interval':'daily','maturity':'10year','name':'yield10y'}\n",
        "#                             ],\n",
        "#           },\n",
        "#          'Commodities':['SPY']\n",
        "#          }\n",
        "\n",
        "# get full data history\n",
        "# df_new = get_consolidated_crypto_data(stock,key)\n",
        "\n",
        "\n",
        "# get the trained model\n",
        "# model = keras.models.load_model(f'/content/drive/MyDrive/Colab Notebooks/models/model_BTC_log_features_optimized')\n",
        "\n",
        "# # Get the features used in the final model\n",
        "# df_btc_features = pd.read_pickle(f'/content/drive/MyDrive/Colab Notebooks/data/{stock}_market_data_features.pickle')\n",
        "# features = [f for f in df_btc_features.columns if f not in ['symbol']]\n",
        "\n",
        "\n",
        "# # initial hist file\n",
        "# start_date = '2021-01-01'\n",
        "# df_tmp = df_new.loc[df_new.index <=start_date]\n",
        "# df_hist_new =roll_predictions2(df_tmp)\n",
        "# df_hist_new.to_pickle(f'/content/drive/MyDrive/Colab Notebooks/data/{stock}_hist2.pickle')\n",
        "\n",
        "# # backtest for sep 2021\n",
        "# datelist = pd.date_range(start_date, periods=270).tolist()\n",
        "# for dt in datelist:\n",
        "\n",
        "#   print(dt)\n",
        "#   print(\"=\"*60)\n",
        "#   df_tmp = df_new.loc[df_new.index <=dt]\n",
        "#   df_orig = df_tmp.copy()\n",
        "\n",
        "#   # transform\n",
        "#   print(\"=> log transpose\\n\")\n",
        "#   transform_stationary(df_tmp,features_to_transform,transform)\n",
        "\n",
        "#   #prepare\n",
        "#   print(\"=>prepare data\\n\")\n",
        "#   idx_dates, array_y, array_X = prepare_data(df_tmp,n_steps,features)\n",
        "\n",
        "#   # scale the input and outputs\n",
        "#   print(\"=>scale data\\n\")\n",
        "#   scaler_X = MinMaxScaler(feature_range=(0,1))\n",
        "#   scaled_X = scaler_X.fit_transform(array_X)\n",
        "#   scaler_y = MinMaxScaler(feature_range=(0,1))\n",
        "#   scaled_y = scaler_y.fit_transform(array_y)\n",
        "\n",
        "\n",
        "#   # make predictions\n",
        "#   print(\"=>make predictions\\n\")\n",
        "#   # df_pred =make_predictions(model,scaler_y,scaled_X,n_steps,len(features),n_predict,df_tmp.index.max()-datetime.timedelta(days=1))\n",
        "#   df_pred =make_predictions(model,scaler_y,scaled_X,n_steps,len(features),n_predict,df_tmp.index.max())\n",
        "\n",
        "#   # get the previous df_hist data\n",
        "#   print(\"=>get previous file\\n\")\n",
        "#   df_hist = pd.read_pickle(f'/content/drive/MyDrive/Colab Notebooks/data/{stock}_hist2.pickle')\n",
        "\n",
        "#   # update the hist file with yesterdays close price, and add the new predictions\n",
        "#   print(\"=>roll file forward\\n\")\n",
        "#   df_hist_new =roll_predictions2(df_orig,df_pred,df_hist)\n",
        "#   df_hist_new.to_pickle(f'/content/drive/MyDrive/Colab Notebooks/data/{stock}_hist2.pickle')\n",
        "\n",
        "\n",
        "# # plot\n",
        "# plot_actual_predicted2(stock,df_hist_new)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE5PK_GOvxwc"
      },
      "source": [
        "## BLX.TO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8DgDxX6v5bH"
      },
      "source": [
        "stock = 'BLX.TO'\n",
        "features_to_transform = ['open','close','high','low']\n",
        "transform = 'log'\n",
        "n_steps = 40\n",
        "n_predict = 5\n",
        "\n",
        "config ={'Economic':\n",
        "         {'TREASURY_YIELD':[{'interval':'daily','maturity':'3month','name':'yield3m'}]},\n",
        "         'Technical':{\n",
        "           'BBANDS':{'interval':'daily','time_period':20},\n",
        "           'MACD':{'interval':'daily','time_period':None}\n",
        "           },\n",
        "         'Commodities':['PBD']\n",
        "         }\n",
        "df_new = get_consolidated_stock_data(stock,key,config,'full')\n",
        "\n",
        "\n",
        "# shift features to match the model\n",
        "df_new = shift_features(df_new,{'yield3m': -60})\n",
        "\n",
        "# get the features\n",
        "features = pd.read_pickle(f'/content/drive/MyDrive/Colab Notebooks/data/{stock}_market_data.pickle')\n",
        "features = features.columns\n",
        "\n",
        "# get the model\n",
        "model = keras.models.load_model(f'/content/drive/MyDrive/Colab Notebooks/models/model_{stock}_log_features_optimized')\n",
        "\n",
        "# initial hist file\n",
        "start_date = '2021-1-1'\n",
        "num_rows = df_new.loc[df_new.index > start_date].shape[0]\n",
        "\n",
        "df_tmp = df_new.loc[df_new.index <=start_date]\n",
        "df_hist_new =roll_predictions(df_tmp)\n",
        "df_hist_new.to_pickle(f'/content/drive/MyDrive/Colab Notebooks/data/{stock}_hist.pickle')\n",
        "\n",
        "# backtest for sep 2021\n",
        "datelist = pd.date_range(start_date, periods=266).tolist()\n",
        "for dt in datelist:\n",
        "\n",
        "  print(dt)\n",
        "  print(\"=\"*60)\n",
        "  df_tmp = df_new.loc[df_new.index <=dt]\n",
        "  df_orig = df_tmp.copy()\n",
        "\n",
        "  # transform\n",
        "  print(\"=> log transpose\\n\")\n",
        "  transform_stationary(df_tmp,features_to_transform,transform)\n",
        "\n",
        "  #prepare\n",
        "  print(\"=>prepare data\\n\")\n",
        "  idx_dates, array_y, array_X = prepare_data(df_tmp,n_steps,features)\n",
        "\n",
        "  # scale the input and outputs\n",
        "  print(\"=>scale data\\n\")\n",
        "  scaler_X = MinMaxScaler(feature_range=(0,1))\n",
        "  scaled_X = scaler_X.fit_transform(array_X)\n",
        "  scaler_y = MinMaxScaler(feature_range=(0,1))\n",
        "  scaled_y = scaler_y.fit_transform(array_y)\n",
        "\n",
        "\n",
        "  # make predictions\n",
        "  print(\"=>make predictions\\n\")\n",
        "  df_pred =make_predictions(model,scaler_y,scaled_X,n_steps,len(features),n_predict,df_tmp.index.max())\n",
        "\n",
        "  # get the previous df_hist data\n",
        "  print(\"=>get previous file\\n\")\n",
        "  df_hist = pd.read_pickle(f'/content/drive/MyDrive/Colab Notebooks/data/{stock}_hist.pickle')\n",
        "\n",
        "  # update the hist file with yesterdays close price, and add the new predictions\n",
        "  print(\"=>roll file forward\\n\")\n",
        "  df_hist_new =roll_predictions(df_orig,df_pred,df_hist)\n",
        "  df_hist_new.to_pickle(f'/content/drive/MyDrive/Colab Notebooks/data/{stock}_hist.pickle')\n",
        "\n",
        "\n",
        "# plot\n",
        "plot_actual_predicted(stock,df_hist_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1Cdo7L-1OJLn",
        "outputId": "fac9ddf2-e737-4642-c1bf-1082fabe0baa"
      },
      "source": [
        "# BLX.TO - with naive model\n",
        "stock = 'BLX.TO'\n",
        "features_to_transform = ['open','close','high','low']\n",
        "transform = 'log'\n",
        "n_steps = 40\n",
        "n_predict = 5\n",
        "\n",
        "config ={'Economic':\n",
        "         {'TREASURY_YIELD':[{'interval':'daily','maturity':'3month','name':'yield3m'}]},\n",
        "         'Technical':{\n",
        "           'BBANDS':{'interval':'daily','time_period':20},\n",
        "           'MACD':{'interval':'daily','time_period':None}\n",
        "           },\n",
        "         'Commodities':['PBD']\n",
        "         }\n",
        "# df_new = get_consolidated_stock_data(stock,key,config,'full')\n",
        "\n",
        "\n",
        "# # shift features to match the model\n",
        "# df_new = shift_features(df_new,{'yield3m': -60})\n",
        "\n",
        "# # get the features\n",
        "# features = pd.read_pickle(f'/content/drive/MyDrive/Colab Notebooks/data/{stock}_market_data.pickle')\n",
        "# features = features.columns\n",
        "\n",
        "# # get the model\n",
        "# model = keras.models.load_model(f'/content/drive/MyDrive/Colab Notebooks/models/model_{stock}_log_features_optimized')\n",
        "\n",
        "\n",
        "# initial hist file\n",
        "start_date = '2021-01-01'\n",
        "df_tmp = df_new.loc[df_new.index <=start_date]\n",
        "df_hist_new =roll_predictions2(df_tmp)\n",
        "df_hist_new.to_pickle(f'/content/drive/MyDrive/Colab Notebooks/data/{stock}_hist2.pickle')\n",
        "\n",
        "# backtest for sep 2021\n",
        "datelist = pd.date_range(start_date, periods=2,freq='B').tolist()\n",
        "for dt in datelist:\n",
        "\n",
        "  print(dt)\n",
        "  print(\"=\"*60)\n",
        "  df_tmp = df_new.loc[df_new.index <=dt]\n",
        "  df_orig = df_tmp.copy()\n",
        "\n",
        "  # transform\n",
        "  print(\"=> log transpose\\n\")\n",
        "  transform_stationary(df_tmp,features_to_transform,transform)\n",
        "\n",
        "  #prepare\n",
        "  print(\"=>prepare data\\n\")\n",
        "  idx_dates, array_y, array_X = prepare_data(df_tmp,n_steps,features)\n",
        "\n",
        "  # scale the input and outputs\n",
        "  print(\"=>scale data\\n\")\n",
        "  scaler_X = MinMaxScaler(feature_range=(0,1))\n",
        "  scaled_X = scaler_X.fit_transform(array_X)\n",
        "  scaler_y = MinMaxScaler(feature_range=(0,1))\n",
        "  scaled_y = scaler_y.fit_transform(array_y)\n",
        "\n",
        "\n",
        "  # make predictions\n",
        "  print(\"=>make predictions\\n\")\n",
        "  # df_pred =make_predictions(model,scaler_y,scaled_X,n_steps,len(features),n_predict,df_tmp.index.max()-datetime.timedelta(days=1))\n",
        "  df_pred =make_predictions(model,scaler_y,scaled_X,n_steps,len(features),n_predict,df_tmp.index.max())\n",
        "\n",
        "  # get the previous df_hist data\n",
        "  print(\"=>get previous file\\n\")\n",
        "  df_hist = pd.read_pickle(f'/content/drive/MyDrive/Colab Notebooks/data/{stock}_hist2.pickle')\n",
        "\n",
        "  # update the hist file with yesterdays close price, and add the new predictions\n",
        "  print(\"=>roll file forward\\n\")\n",
        "  df_hist_new =roll_predictions2(df_orig,df_pred,df_hist)\n",
        "  df_hist_new.to_pickle(f'/content/drive/MyDrive/Colab Notebooks/data/{stock}_hist2.pickle')\n",
        "\n",
        "\n",
        "# plot\n",
        "plot_actual_predicted2(stock,df_hist_new)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-01-01 00:00:00\n",
            "============================================================\n",
            "=> log transpose\n",
            "\n",
            "\n",
            "\n",
            "Augmented Dicky Fuller Test for Stationarity\n",
            "============================================================\n",
            "ADF Statistic: -2.35\n",
            "Critial Values:\n",
            "   1%, -3.43 => stationary\n",
            "Critial Values:\n",
            "   5%, -2.86 => stationary\n",
            "Critial Values:\n",
            "   10%, -2.57 => stationary\n",
            "=>prepare data\n",
            "\n",
            "\n",
            "Data Preparation\n",
            "============================================================\n",
            "=> 10 Features\n",
            "=> Input Dimensions :(40, 10)\n",
            "\n",
            "\n",
            "=>scale data\n",
            "\n",
            "=>make predictions\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>get previous file\n",
            "\n",
            "=>roll file forward\n",
            "\n",
            "2021-01-04 00:00:00\n",
            "============================================================\n",
            "=> log transpose\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Augmented Dicky Fuller Test for Stationarity\n",
            "============================================================\n",
            "ADF Statistic: -2.42\n",
            "Critial Values:\n",
            "   1%, -3.43 => stationary\n",
            "Critial Values:\n",
            "   5%, -2.86 => stationary\n",
            "Critial Values:\n",
            "   10%, -2.57 => stationary\n",
            "=>prepare data\n",
            "\n",
            "\n",
            "Data Preparation\n",
            "============================================================\n",
            "=> 10 Features\n",
            "=> Input Dimensions :(40, 10)\n",
            "\n",
            "\n",
            "=>scale data\n",
            "\n",
            "=>make predictions\n",
            "\n",
            "=>get previous file\n",
            "\n",
            "=>roll file forward\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.DatetimeEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1609632000000000000",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.DatetimeEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.DatetimeEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: Timestamp('2021-01-03 00:00:00')",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/datetimes.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: Timestamp('2021-01-03 00:00:00')",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-146-0bfc54f25fcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[0;31m# update the hist file with yesterdays close price, and add the new predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=>roll file forward\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0mdf_hist_new\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mroll_predictions2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_orig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_hist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0mdf_hist_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'/content/drive/MyDrive/Colab Notebooks/data/{stock}_hist2.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-120-a3bfdc23d965>\u001b[0m in \u001b[0;36mroll_predictions2\u001b[0;34m(df_new, df_pred, df_hist)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0myesterday\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0myesterdays_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myesterday\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'close'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mnaive_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myesterday\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'close'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# update df_hist with yesterdays_close,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    871\u001b[0m                     \u001b[0;31m# AttributeError for IntervalTree get_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    784\u001b[0m                 \u001b[0;31m# We don't need to check for tuples here because those are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0;31m#  caught by the _is_nested_tuple_indexer check above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m                 \u001b[0msection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0;31m# We should never have a scalar section here, because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;31m# GH#5667 this will fail if the label is not present in the axis.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_lowerdim_multi_index_axis0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mxs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   3491\u001b[0m             \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3493\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3495\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/datetimes.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_key\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_maybe_cast_for_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTimestamp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: Timestamp('2021-01-03 00:00:00')"
          ]
        }
      ]
    }
  ]
}